{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TRAIN_DATA_PATH = '../data/train_data.csv'\n",
    "TRAIN_LABELS_PATH = '../data/train_labels.csv'\n",
    "TEST_DATA_PATH = '../data/test_data.csv'\n",
    "EXPORT_PATH = '../data/processed/1_preprocessed_data.pkl'\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_FEATURES_SELECT = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pd.options.display.max_rows = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(X_train, X_test, n_features_select=2000):\n",
    "    \"\"\"\n",
    "    Preprocess features: variance threshold, scaling, feature selection\n",
    "    \n",
    "    Parameters:\n",
    "    * X_train (np.array): Training features\n",
    "    * X_test (np.array): Test features\n",
    "    * n_features_select (int): Number of features to select\n",
    "    \n",
    "    Returns: tuple of (X_train_processed, X_test_processed, feature_selector, scaler)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Variance threshold\n",
    "    variance_selector = VarianceThreshold(threshold=0.01)\n",
    "    X_train_var = variance_selector.fit_transform(X_train)\n",
    "    X_test_var = variance_selector.transform(X_test)\n",
    "    \n",
    "    # Scaling\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_var)\n",
    "    X_test_scaled = scaler.transform(X_test_var)\n",
    "    \n",
    "    # Feature selection\n",
    "    n_features_to_select = min(n_features_select, X_train_scaled.shape[1])\n",
    "    feature_selector = SelectKBest(score_func=f_classif, k=n_features_to_select)\n",
    "    X_train_selected = feature_selector.fit_transform(X_train_scaled, y_train)\n",
    "    X_test_selected = feature_selector.transform(X_test_scaled)\n",
    "    \n",
    "    return X_train_selected, X_test_selected, feature_selector, scaler, variance_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_class_imbalance(X_train, y_train, random_state=42):\n",
    "    \"\"\"\n",
    "    Handle class imbalance using SMOTE\n",
    "    \n",
    "    Parameters:\n",
    "    * X_train (np.array): Training features\n",
    "    * y_train (np.array): Training labels\n",
    "    * random_state (int): Random state for reproducibility\n",
    "    \n",
    "    Returns: tuple of (X_train_balanced, y_train_balanced, smote)\n",
    "    \"\"\"\n",
    "    \n",
    "    smote = SMOTE(random_state=random_state, k_neighbors=3)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    return X_train_balanced, y_train_balanced, smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_data = pd.read_csv(TRAIN_DATA_PATH, header=None)\n",
    "train_labels = pd.read_csv(TRAIN_LABELS_PATH, header=None)\n",
    "test_data = pd.read_csv(TEST_DATA_PATH, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3750, 10000)\n",
      "y_train shape: (3750,)\n",
      "X_test shape: (1250, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "X_train = train_data.values\n",
    "y_train = train_labels.values.ravel()\n",
    "X_test = test_data.values\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 10000\n",
      "After variance threshold: 0\n",
      "After feature selection: 2000\n",
      "Feature reduction: 80.0%\n"
     ]
    }
   ],
   "source": [
    "# Preprocess features\n",
    "X_train_processed, X_test_processed, feature_selector, scaler, variance_selector = \\\n",
    "    preprocess_features(X_train, X_test, N_FEATURES_SELECT)\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"After variance threshold: {X_train.shape[1] - variance_selector.get_support().sum()}\")\n",
    "print(f\"After feature selection: {X_train_processed.shape[1]}\")\n",
    "print(f\"Feature reduction: {((X_train.shape[1] - X_train_processed.shape[1]) / X_train.shape[1] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class -1 count: 373\n",
      "Class +1 count: 3377\n",
      "Imbalance ratio: 9.05:1\n",
      "Minority class percentage: 9.9%\n"
     ]
    }
   ],
   "source": [
    "# Analyze class distribution\n",
    "unique_labels = np.unique(y_train)\n",
    "class_counts = np.array([np.sum(y_train == -1), np.sum(y_train == 1)])\n",
    "imbalance_ratio = class_counts[1] / class_counts[0]\n",
    "\n",
    "print(f\"Class -1 count: {class_counts[0]}\")\n",
    "print(f\"Class +1 count: {class_counts[1]}\")\n",
    "print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"Minority class percentage: {class_counts[0] / len(y_train) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set: 3750 samples\n",
      "SMOTE-balanced training set: 6754 samples\n",
      "Class distribution after SMOTE:\n",
      "  Class -1: 3377 samples\n",
      "  Class +1: 3377 samples\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE\n",
    "X_train_balanced, y_train_balanced, smote = handle_class_imbalance(X_train_processed, y_train, RANDOM_STATE)\n",
    "\n",
    "print(f\"Original training set: {X_train_processed.shape[0]} samples\")\n",
    "print(f\"SMOTE-balanced training set: {X_train_balanced.shape[0]} samples\")\n",
    "print(f\"Class distribution after SMOTE:\")\n",
    "print(f\"  Class -1: {np.sum(y_train_balanced == -1)} samples\")\n",
    "print(f\"  Class +1: {np.sum(y_train_balanced == 1)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels: [-1  1]\n",
      "Transformed labels: [-1  1]\n",
      "Sample of transformed y_train: [1 1 1 1 1 0 1 1 1 1]\n",
      "Unique values in transformed: [0 1]\n",
      "Mapping: {-1: 0, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "# Label encoding for XGBoost\n",
    "xgb_label_encoder = LabelEncoder()\n",
    "y_train_transformed = xgb_label_encoder.fit_transform(y_train)\n",
    "y_train_balanced_transformed = xgb_label_encoder.transform(y_train_balanced)\n",
    "\n",
    "print(f\"Original labels: {unique_labels}\")\n",
    "print(f\"Transformed labels: {xgb_label_encoder.classes_}\")\n",
    "print(f\"Sample of transformed y_train: {y_train_transformed[:10]}\")\n",
    "print(f\"Unique values in transformed: {np.unique(y_train_transformed)}\")\n",
    "print(f\"Mapping: {dict(zip([int(x) for x in xgb_label_encoder.classes_], range(len(xgb_label_encoder.classes_))))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {-1: 5.02680965147453, 1: 0.5552265324252295}\n",
      "XGBoost scale_pos_weight: 9.05\n"
     ]
    }
   ],
   "source": [
    "# Class weights\n",
    "class_weights = compute_class_weight('balanced', classes=unique_labels, y=y_train)\n",
    "class_weight_dict = dict(zip(unique_labels, class_weights))\n",
    "scale_pos_weight = class_counts[1] / class_counts[0]\n",
    "\n",
    "class_weight_dict = {int(k): float(v) for k, v in class_weight_dict.items()}\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "print(f\"XGBoost scale_pos_weight: {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to ../data/processed/1_preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# Create processed data dictionary\n",
    "processed_data = {\n",
    "    'X_train_original': X_train_processed,\n",
    "    'y_train_original': y_train,\n",
    "    'X_train_balanced': X_train_balanced,\n",
    "    'y_train_balanced': y_train_balanced,\n",
    "    'y_train_transformed': y_train_transformed,\n",
    "    'y_train_balanced_transformed': y_train_balanced_transformed,\n",
    "    'X_test': X_test_processed,\n",
    "    'feature_selector': feature_selector,\n",
    "    'scaler': scaler,\n",
    "    'variance_selector': variance_selector,\n",
    "    'xgb_label_encoder': xgb_label_encoder,\n",
    "    'class_weight_dict': class_weight_dict,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'smote': smote\n",
    "}\n",
    "\n",
    "# Save to pickle\n",
    "import os\n",
    "\n",
    "# Ensure the processed data directory exists\n",
    "export_dir = os.path.dirname(EXPORT_PATH)\n",
    "if not os.path.exists(export_dir):\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "with open(EXPORT_PATH, 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "\n",
    "print(f\"Preprocessed data saved to {EXPORT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek-mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
